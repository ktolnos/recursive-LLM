{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-06T21:46:30.495160Z",
     "start_time": "2025-11-06T21:46:24.598783Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.cuda.tunable import enable\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "7aeff5bf78ff23b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T23:55:02.948181Z",
     "start_time": "2025-11-06T23:55:02.688810Z"
    }
   },
   "cell_type": "code",
   "source": "len(model.model.layers)",
   "id": "aba7089ee22cb00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T21:53:48.869461Z",
     "start_time": "2025-11-06T21:53:38.364322Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/100...\n",
      "Processed 2/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 3/100...\n",
      "Processed 4/100...\n",
      "Correct answer: D, predicted answer: C\n",
      "Processed 5/100...\n",
      "Correct answer: B, predicted answer: D\n",
      "Processed 6/100...\n",
      "Processed 7/100...\n",
      "Processed 8/100...\n",
      "Processed 9/100...\n",
      "Processed 10/100...\n",
      "Processed 11/100...\n",
      "Processed 12/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 13/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 14/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 15/100...\n",
      "Processed 16/100...\n",
      "Processed 17/100...\n",
      "Correct answer: B, predicted answer: A\n",
      "Processed 18/100...\n",
      "Processed 19/100...\n",
      "Correct answer: C, predicted answer: A\n",
      "Processed 20/100...\n",
      "Processed 21/100...\n",
      "Processed 22/100...\n",
      "Processed 23/100...\n",
      "Correct answer: C, predicted answer: A\n",
      "Processed 24/100...\n",
      "Processed 25/100...\n",
      "Correct answer: D, predicted answer: A\n",
      "Processed 26/100...\n",
      "Processed 27/100...\n",
      "Processed 28/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 29/100...\n",
      "Processed 30/100...\n",
      "Correct answer: D, predicted answer: B\n",
      "Processed 31/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 32/100...\n",
      "Correct answer: A, predicted answer: D\n",
      "Processed 33/100...\n",
      "Processed 34/100...\n",
      "Correct answer: A, predicted answer: C\n",
      "Processed 35/100...\n",
      "Correct answer: A, predicted answer: B\n",
      "Processed 36/100...\n",
      "Correct answer: B, predicted answer: A\n",
      "Processed 37/100...\n",
      "Correct answer: B, predicted answer: C\n",
      "Processed 38/100...\n",
      "Processed 39/100...\n",
      "Processed 40/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 41/100...\n",
      "Correct answer: A, predicted answer: C\n",
      "Processed 42/100...\n",
      "Processed 43/100...\n",
      "Correct answer: D, predicted answer: A\n",
      "Processed 44/100...\n",
      "Correct answer: 2, predicted answer: 1\n",
      "Processed 45/100...\n",
      "Correct answer: B, predicted answer: A\n",
      "Processed 46/100...\n",
      "Correct answer: C, predicted answer: A\n",
      "Processed 47/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 48/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 49/100...\n",
      "Correct answer: A, predicted answer: D\n",
      "Processed 50/100...\n",
      "Processed 51/100...\n",
      "Processed 52/100...\n",
      "Correct answer: A, predicted answer: D\n",
      "Processed 53/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 54/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 55/100...\n",
      "Correct answer: B, predicted answer: D\n",
      "Processed 56/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 57/100...\n",
      "Processed 58/100...\n",
      "Processed 59/100...\n",
      "Correct answer: C, predicted answer: A\n",
      "Processed 60/100...\n",
      "Processed 61/100...\n",
      "Processed 62/100...\n",
      "Processed 63/100...\n",
      "Processed 64/100...\n",
      "Correct answer: D, predicted answer: A\n",
      "Processed 65/100...\n",
      "Processed 66/100...\n",
      "Correct answer: A, predicted answer: B\n",
      "Processed 67/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 68/100...\n",
      "Correct answer: B, predicted answer: A\n",
      "Processed 69/100...\n",
      "Processed 70/100...\n",
      "Processed 71/100...\n",
      "Correct answer: C, predicted answer: D\n",
      "Processed 72/100...\n",
      "Correct answer: B, predicted answer: C\n",
      "Processed 73/100...\n",
      "Processed 74/100...\n",
      "Processed 75/100...\n",
      "Correct answer: A, predicted answer: D\n",
      "Processed 76/100...\n",
      "Processed 77/100...\n",
      "Correct answer: B, predicted answer: A\n",
      "Processed 78/100...\n",
      "Processed 79/100...\n",
      "Processed 80/100...\n",
      "Processed 81/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 82/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 83/100...\n",
      "Correct answer: D, predicted answer: A\n",
      "Processed 84/100...\n",
      "Correct answer: D, predicted answer: C\n",
      "Processed 85/100...\n",
      "Processed 86/100...\n",
      "Correct answer: A, predicted answer: D\n",
      "Processed 87/100...\n",
      "Correct answer: D, predicted answer: B\n",
      "Processed 88/100...\n",
      "Correct answer: C, predicted answer: A\n",
      "Processed 89/100...\n",
      "Processed 90/100...\n",
      "Correct answer: A, predicted answer: C\n",
      "Processed 91/100...\n",
      "Correct answer: D, predicted answer: A\n",
      "Processed 92/100...\n",
      "Processed 93/100...\n",
      "Correct answer: A, predicted answer: B\n",
      "Processed 94/100...\n",
      "Processed 95/100...\n",
      "Correct answer: C, predicted answer: B\n",
      "Processed 96/100...\n",
      "Processed 97/100...\n",
      "Correct answer: B, predicted answer: C\n",
      "Processed 98/100...\n",
      "Processed 99/100...\n",
      "Processed 100/100...\n",
      "\n",
      "Accuracy: 47.00% (47/100)\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "\n",
    "# dataset = load_dataset(\"opencompass/AIME2025\", \"AIME2025-I\", split=\"test\")\n",
    "dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# it = iter(dataset)\n",
    "# next(it)\n",
    "# item = next(it)\n",
    "dataset = dataset.take(100)\n",
    "\n",
    "for item in dataset:\n",
    "    question = item[\"question\"]\n",
    "    # reference_answer = item[\"answer\"]\n",
    "    choices = item[\"choices\"]\n",
    "    answer_key = item[\"answerKey\"]\n",
    "\n",
    "    for i, choice in enumerate(choices['text']):\n",
    "        label = choices['label'][i]\n",
    "        question += f\"{label}. {choice}\\n\"\n",
    "\n",
    "    messages= [\n",
    "        {\n",
    "            'content': 'You are a helpful assistant. Answer with a SINGLE letter without any additional formatting.',\n",
    "            'role': 'system',\n",
    "        },\n",
    "        {\n",
    "            'content': question,\n",
    "            'role': 'user',\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, do_sample=False)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predicted_answer = generated_text[-1]\n",
    "\n",
    "    if answer_key.lower() == predicted_answer.lower():\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f\"Correct answer: {answer_key}, predicted answer: {predicted_answer}\")\n",
    "    total += 1\n",
    "\n",
    "    print(f\"Processed {total}/{len(dataset)}...\")\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n"
   ],
   "id": "6aed1edbb3eb72b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b0beb2fe62251177"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
